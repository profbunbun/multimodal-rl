@startuml classes
set namespaceSeparator none
class ".T_destination" as .T_destination {
}
class "collections.OrderedDict" as collections.OrderedDict {
  {abstract}move_to_end(key, last)
}
class "collections.defaultdict" as collections.defaultdict {
  default_factory : NoneType
}
class "multimodal-rl.sumo_mmrl.agent.agent.Agent" as multimodal-rl.sumo_mmrl.agent.agent.Agent {
  batch_size : NoneType
  criterion
  device
  direction_choices : list
  epsilon_decay : NoneType
  epsilon_max : NoneType
  epsilon_min : NoneType
  exploration_strategy
  gamma : NoneType
  learning_rate : NoneType
  memory
  memory_size : NoneType
  optimizer
  path
  policy_net : NoneType
  soft_update_factor : NoneType
  target_net : NoneType
  wandb_run
  choose_action(state, options)
  decay()
  generate_config_id()
  get_epsilon()
  get_exploration_stats()
  get_model_state_dict_as_string(model)
  get_optimizer_state_dict_as_string(optimizer)
  hard_update()
  load_model(artifact_name)
  perform_training_step(state, action, reward, next_state, done)
  remember(state, action, reward, next_state, done)
  replay(batch_size)
  save_model(episode_num)
  set_hyperparameters(learning_rate, gamma, epsilon_decay)
  soft_update()
}
class "multimodal-rl.sumo_mmrl.agent.dqn.DQN" as multimodal-rl.sumo_mmrl.agent.dqn.DQN {
  activation_function
  layers
  forward(x_net)
  get_activation_function(name)
  init_weights(m)
}
class "multimodal-rl.sumo_mmrl.agent.exploration.Explorer" as multimodal-rl.sumo_mmrl.agent.exploration.Explorer {
  decay_rate : float
  direction_choices : list
  epsilon : int
  epsilon_min : float
  exploit_count : int
  explore_count : int
  last_reward : NoneType
  policy_net
  choose_action(state, options)
  exploit(state)
  explore()
  get_exploration_stats()
  save_stats(filename)
  update_epsilon()
}
class "multimodal-rl.sumo_mmrl.agent.memory.Memory" as multimodal-rl.sumo_mmrl.agent.memory.Memory {
  memory
  memory_size
  position : int
  remember(state, action, reward, next_state, done)
  replay_batch(batch_size)
}
class "multimodal-rl.sumo_mmrl.environment.bus_stop.StopFinder" as multimodal-rl.sumo_mmrl.environment.bus_stop.StopFinder {
  con : NoneType
  find_begin_stop(begin_loc, loc_dic, con)
  find_bus_locs()
  find_end_stop(end_loc, loc_dic, con)
  get_line(stop_id)
  get_line_route(con)
  get_stop_dists(loc, loc_dic)
  manhat_dist(x1, y1, x2, y2)
}
class "multimodal-rl.sumo_mmrl.environment.connect.SUMOConnection" as multimodal-rl.sumo_mmrl.environment.connect.SUMOConnection {
  label : str
  sumo_ : NoneType
  sumo_cmd : NoneType, list
  sumocfg : str
  busstopCheck()
  close()
  connect_gui()
  connect_libsumo_no_gui()
  connect_no_gui()
  get_edge_list()
  get_junction_list()
  get_lane_list()
}
class "multimodal-rl.sumo_mmrl.environment.env.Env" as multimodal-rl.sumo_mmrl.environment.env.Env {
  accumulated_reward : int
  agent_step : int
  best_choice
  bussroute : list
  config
  destination_edge : NoneType
  distcheck : int
  edge_distance : NoneType
  edge_position : dict
  epsilon_hist : list
  finder
  graph_path
  life
  make_choice_flag : bool
  num_of_vehicles
  obs
  old_dist : NoneType, int
  old_edge : NoneType
  out_mask
  p_index : int
  parser
  path
  penalty
  person : NoneType
  person_manager
  reward_calculator
  rewards : list
  ride_selector
  smoothing_window
  stage : str
  stage_manager
  step_manager
  steps : int
  sumo : NoneType
  sumo_con
  sumo_config_path
  types_of_passengers
  vehicle : NoneType
  vehicle_manager
  close(episode, accu, current_epsilon)
  get_best_choice()
  get_destination_edge_id()
  get_global_step()
  get_life()
  get_out_lanes()
  get_steps_per_episode()
  get_vehicle_location_edge_id()
  render(mode)
  reset()
  step(action, validator)
}
class "multimodal-rl.sumo_mmrl.environment.net_parser.NetParser" as multimodal-rl.sumo_mmrl.environment.net_parser.NetParser {
  sumocfg
  get_edge_index()
  get_edge_pos_dic()
  get_edges_info()
  get_junctions()
  get_length_dic()
  get_max_manhattan()
  get_out_dic()
  get_route_edges()
  net_minmax()
  parse_net_files()
}
class "multimodal-rl.sumo_mmrl.environment.observation.Observation" as multimodal-rl.sumo_mmrl.environment.observation.Observation {
  max_manhat_dist
  max_x
  max_y
  min_x
  min_y
  out_mask
  get_state(sumo, step, vehicle, destination_loc, life, distcheck)
  manhat_dist(x1, y1, x2, y2)
  normalize(value, min_value, max_value)
}
class "multimodal-rl.sumo_mmrl.environment.outmask.OutMask" as multimodal-rl.sumo_mmrl.environment.outmask.OutMask {
  get_outmask(vedge, pedge, choices, edge_position)
  get_outmask_valid(choices)
  manhat_dist(x1, y1, x2, y2)
}
class "multimodal-rl.sumo_mmrl.environment.person.Person" as multimodal-rl.sumo_mmrl.environment.person.Person {
  destination
  edge_position
  index_dict
  new_lane
  person_id
  sumo
  get_destination()
  get_road()
  get_type()
  location()
  remove_person()
}
class "multimodal-rl.sumo_mmrl.environment.person_manager.PersonManager" as multimodal-rl.sumo_mmrl.environment.person_manager.PersonManager {
  edge_position
  index_dict
  num_of_people
  sumo
  create_people()
}
class "multimodal-rl.sumo_mmrl.environment.plot_util.Plotter" as multimodal-rl.sumo_mmrl.environment.plot_util.Plotter {
  find_first_valid_index(data)
  plot_learning(x, smoothed_rewards, epsilons, filename)
}
class "multimodal-rl.sumo_mmrl.environment.reward_calculator.RewardCalculator" as multimodal-rl.sumo_mmrl.environment.reward_calculator.RewardCalculator {
  edge_position
  calculate_reward(old_dist, edge_distance, stage, destination_edge, vedge, make_choice_flag, life)
}
class "multimodal-rl.sumo_mmrl.environment.ride_select.RideSelect" as multimodal-rl.sumo_mmrl.environment.ride_select.RideSelect {
  make_vehic_atribs_dic(vehicle_array)
  select(vehicle_array, person)
}
class "multimodal-rl.sumo_mmrl.environment.stage_manager.StageManager" as multimodal-rl.sumo_mmrl.environment.stage_manager.StageManager {
  bussroute
  edge_position
  finder
  stage : str
  sumo
  get_initial_stage()
  update_stage(current_stage, destination_edge, vedge, person)
}
class "multimodal-rl.sumo_mmrl.environment.step_manager.StepManager" as multimodal-rl.sumo_mmrl.environment.step_manager.StepManager {
  sumo_interface
  null_step(vehicle, make_choice_flag, old_edge)
  perform_step(vehicle, action, destination_edge)
}
class "multimodal-rl.sumo_mmrl.environment.utils.Utils" as multimodal-rl.sumo_mmrl.environment.utils.Utils {
  manhattan_distance(x1, y1, x2, y2)
}
class "multimodal-rl.sumo_mmrl.environment.vehicle.Vehicle" as multimodal-rl.sumo_mmrl.environment.vehicle.Vehicle {
  cur_loc
  current_lane
  direction_choices : list
  edge_position
  index_dict
  out_dict
  sumo
  vehicle_id
  get_lane()
  get_lane_id()
  get_out_dict()
  get_road()
  get_type()
  location()
  pickup()
  random_relocate()
  set_destination(action, destination_edge)
  teleport(dest)
}
class "multimodal-rl.sumo_mmrl.environment.vehicle_manager.VehicleManager" as multimodal-rl.sumo_mmrl.environment.vehicle_manager.VehicleManager {
  edge_position
  index_dict
  num_of_vehicles
  out_dict
  sumo
  create_vehicles()
}
class "multimodal-rl.sumo_mmrl.utilities.env_utils.Utils" as multimodal-rl.sumo_mmrl.utilities.env_utils.Utils {
  ensure_directory_exists(directory)
  manhattan_distance(x1, y1, x2, y2)
  plot_learning_curve(x, rewards, epsilons, file_name)
  smooth_data(data, window_size)
}
class "multimodal-rl.sumo_mmrl.utilities.plot_util.Plotter" as multimodal-rl.sumo_mmrl.utilities.plot_util.Plotter {
  find_first_valid_index(data)
  plot_learning(x, smoothed_rewards, epsilons, filename)
}
class "multimodal-rl.sumo_mmrl.utilities.utils.Utils" as multimodal-rl.sumo_mmrl.utilities.utils.Utils {
  get_next_study_name(base_name)
  load_yaml_config(config_path)
  parse_arguments()
  setup_study(storage_path, pruner)
}
class "torch.nn.modules.container.ModuleList" as torch.nn.modules.container.ModuleList {
  append(module: Module) -> 'ModuleList'
  extend(modules: Iterable[Module]) -> 'ModuleList'
  insert(index: int, module: Module) -> None
  pop(key: Union[int, slice]) -> Module
}
class "torch.nn.modules.loss.HuberLoss" as torch.nn.modules.loss.HuberLoss {
  delta : float
  forward(input: Tensor, target: Tensor) -> Tensor
}
class "torch.nn.modules.loss._Loss" as torch.nn.modules.loss._Loss {
  reduction : str
  reduction : str
}
class "torch.nn.modules.module.Module" as torch.nn.modules.module.Module {
  T_destination
  call_super_init : bool
  dump_patches : bool
  forward : Callable[..., Any]
  training : bool
  training : bool
  add_module(name: str, module: Optional['Module']) -> None
  apply(fn: Callable[['Module'], None]) -> T
  bfloat16() -> T
  buffers(recurse: bool) -> Iterator[Tensor]
  children() -> Iterator['Module']
  cpu() -> T
  cuda(device: Optional[Union[int, device]]) -> T
  double() -> T
  eval() -> T
  extra_repr() -> str
  float() -> T
  get_buffer(target: str) -> 'Tensor'
  get_extra_state() -> Any
  get_parameter(target: str) -> 'Parameter'
  get_submodule(target: str) -> 'Module'
  half() -> T
  ipu(device: Optional[Union[int, device]]) -> T
  load_state_dict(state_dict: Mapping[str, Any], strict: bool)
  modules() -> Iterator['Module']
  named_buffers(prefix: str, recurse: bool, remove_duplicate: bool) -> Iterator[Tuple[str, Tensor]]
  named_children() -> Iterator[Tuple[str, 'Module']]
  named_modules(memo: Optional[Set['Module']], prefix: str, remove_duplicate: bool)
  named_parameters(prefix: str, recurse: bool, remove_duplicate: bool) -> Iterator[Tuple[str, Parameter]]
  parameters(recurse: bool) -> Iterator[Parameter]
  register_backward_hook(hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]) -> RemovableHandle
  register_buffer(name: str, tensor: Optional[Tensor], persistent: bool) -> None
  register_forward_hook(hook: Union[Callable[[T, Tuple[Any, ...], Any], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]]) -> RemovableHandle
  register_forward_pre_hook(hook: Union[Callable[[T, Tuple[Any, ...]], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]]) -> RemovableHandle
  register_full_backward_hook(hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]], prepend: bool) -> RemovableHandle
  register_full_backward_pre_hook(hook: Callable[['Module', _grad_t], Union[None, _grad_t]], prepend: bool) -> RemovableHandle
  register_load_state_dict_post_hook(hook)
  register_module(name: str, module: Optional['Module']) -> None
  register_parameter(name: str, param: Optional[Parameter]) -> None
  register_state_dict_pre_hook(hook)
  requires_grad_(requires_grad: bool) -> T
  set_extra_state(state: Any)
  share_memory() -> T
  state_dict() -> T_destination
  to(device: Optional[Union[int, device]], dtype: Optional[Union[dtype, str]], non_blocking: bool) -> T
  to_empty() -> T
  train(mode: bool) -> T
  type(dst_type: Union[dtype, str]) -> T
  xpu(device: Optional[Union[int, device]]) -> T
  zero_grad(set_to_none: bool) -> None
}
class "torch.optim.optimizer.Optimizer" as torch.optim.optimizer.Optimizer {
  defaults
  param_groups : list
  state
  add_param_group(param_group)
  load_state_dict(state_dict)
  profile_hook_step(func)
  register_step_post_hook(hook: Callable[..., None]) -> RemovableHandle
  register_step_pre_hook(hook: Callable[..., None]) -> RemovableHandle
  state_dict()
  {abstract}step(closure)
  zero_grad(set_to_none: bool)
}
class "torch.optim.rmsprop.RMSprop" as torch.optim.rmsprop.RMSprop {
  step(closure)
}
multimodal-rl.sumo_mmrl.agent.dqn.DQN --|> torch.nn.modules.module.Module
torch.nn.modules.container.ModuleList --|> torch.nn.modules.module.Module
torch.nn.modules.loss.HuberLoss --|> torch.nn.modules.loss._Loss
torch.nn.modules.loss._Loss --|> torch.nn.modules.module.Module
torch.optim.rmsprop.RMSprop --|> torch.optim.optimizer.Optimizer
.T_destination --* torch.nn.modules.module.Module : T_destination
collections.OrderedDict --* collections.OrderedDict : _metadata
collections.OrderedDict --* torch.nn.modules.container.ModuleList : _modules
collections.OrderedDict --* torch.nn.modules.module.Module : _forward_pre_hooks
collections.OrderedDict --* torch.nn.modules.module.Module : _forward_pre_hooks_with_kwargs
collections.OrderedDict --* torch.nn.modules.module.Module : _forward_hooks_with_kwargs
collections.OrderedDict --* torch.nn.modules.module.Module : _state_dict_hooks
collections.OrderedDict --* torch.nn.modules.module.Module : _state_dict_pre_hooks
collections.OrderedDict --* torch.nn.modules.module.Module : _load_state_dict_pre_hooks
collections.OrderedDict --* torch.nn.modules.module.Module : _load_state_dict_post_hooks
collections.OrderedDict --* torch.nn.modules.module.Module : _backward_pre_hooks
collections.OrderedDict --* torch.optim.optimizer.Optimizer : _optimizer_step_pre_hooks
collections.OrderedDict --* torch.optim.optimizer.Optimizer : _optimizer_step_post_hooks
collections.defaultdict --* torch.optim.optimizer.Optimizer : state
multimodal-rl.sumo_mmrl.agent.exploration.Explorer --* multimodal-rl.sumo_mmrl.agent.agent.Agent : exploration_strategy
multimodal-rl.sumo_mmrl.agent.exploration.Explorer --* multimodal-rl.sumo_mmrl.agent.agent.Agent : exploration_strategy
multimodal-rl.sumo_mmrl.agent.memory.Memory --* multimodal-rl.sumo_mmrl.agent.agent.Agent : memory
multimodal-rl.sumo_mmrl.environment.bus_stop.StopFinder --* multimodal-rl.sumo_mmrl.environment.env.Env : finder
multimodal-rl.sumo_mmrl.environment.connect.SUMOConnection --* multimodal-rl.sumo_mmrl.environment.env.Env : sumo_con
multimodal-rl.sumo_mmrl.environment.net_parser.NetParser --* multimodal-rl.sumo_mmrl.environment.env.Env : parser
multimodal-rl.sumo_mmrl.environment.observation.Observation --* multimodal-rl.sumo_mmrl.environment.env.Env : obs
multimodal-rl.sumo_mmrl.environment.outmask.OutMask --* multimodal-rl.sumo_mmrl.environment.env.Env : out_mask
multimodal-rl.sumo_mmrl.environment.outmask.OutMask --* multimodal-rl.sumo_mmrl.environment.observation.Observation : out_mask
multimodal-rl.sumo_mmrl.environment.person_manager.PersonManager --* multimodal-rl.sumo_mmrl.environment.env.Env : person_manager
multimodal-rl.sumo_mmrl.environment.reward_calculator.RewardCalculator --* multimodal-rl.sumo_mmrl.environment.env.Env : reward_calculator
multimodal-rl.sumo_mmrl.environment.ride_select.RideSelect --* multimodal-rl.sumo_mmrl.environment.env.Env : ride_selector
multimodal-rl.sumo_mmrl.environment.stage_manager.StageManager --* multimodal-rl.sumo_mmrl.environment.env.Env : stage_manager
multimodal-rl.sumo_mmrl.environment.step_manager.StepManager --* multimodal-rl.sumo_mmrl.environment.env.Env : step_manager
multimodal-rl.sumo_mmrl.environment.vehicle_manager.VehicleManager --* multimodal-rl.sumo_mmrl.environment.env.Env : vehicle_manager
torch.nn.modules.container.ModuleList --* multimodal-rl.sumo_mmrl.agent.dqn.DQN : layers
torch.nn.modules.loss.HuberLoss --* multimodal-rl.sumo_mmrl.agent.agent.Agent : criterion
torch.optim.rmsprop.RMSprop --* multimodal-rl.sumo_mmrl.agent.agent.Agent : optimizer
torch.optim.rmsprop.RMSprop --* multimodal-rl.sumo_mmrl.agent.agent.Agent : optimizer
@enduml
